{"meta":{"title":"脑洞","subtitle":"框架 结构 混沌 邪","description":null,"author":"mx-1024","url":"http://yoursite.com","root":"/"},"pages":[{"title":"tag","date":"2019-05-15T15:57:20.000Z","updated":"2019-05-15T16:19:58.858Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"推荐系统排序优化迭代的一些经验","slug":"rank优化路径","date":"2020-02-09T02:58:21.000Z","updated":"2020-02-09T17:16:58.980Z","comments":true,"path":"posts/2020/02/09/62333/","link":"","permalink":"http://yoursite.com/posts/2020/02/09/62333/","excerpt":"","text":"rank 路径在几经周折之后，我们几个主要场景的主要排序模型都已经从FM切换到NN上来了。 我们目前主要做的工作还是对常见信息流中图文数据进行ctr预估工作，目前阶段我们还只考虑点击。 迫于懒，一些冠冕堂皇的屁话就省略了，需要的请自行脑补。 1. 历史进程 模型 offline(AUC) Online 时间 Simple DNN 1% 2% 201905 PCTR 4.30% 3.78% 201907 DSIN 6.16% 10.45% 201912 以上对比的baseline都是同时期在线上运行的FM模型，相同的特征集合。 因为后期不同模型逐渐放量后，FM模型占比持续降低，所以对于线上表现而言，表现差异会逐步变大。 Simple NN 之前差不多在一人力的情况下摸索了有10个月。其中线上工程代码相关花费了大量精力。 2. 模型演进 2.1 WDL 上表里之所以没有把这个模型列出来，是因为在这个模型上我们并没有获取到收益……拿出来单讲将算是个铺垫吧（ 我们最开始也是想着尽可能复用更多已有的工作。所以，最开始选择尝试的模型结构就是WDL，也是为了尽可能保留之前FM的效果。模型是使用tensorflow里的DNNLinearCombinedClassifier ，三个隐层分别使用了 [128,32,8] 个ReLU Unites 。 客观来讲，之前我们使用FM代替LR的线上收益大约在1%+左右。WDL的wide侧是一个普通的LR，在特征选择方面我们沿用了之前FM的全部特征，这些特征已经包含了大量的业务背景和人工经验，最悲观的情况下，如果deep侧我们什么都不做，完全平移的话，离线auc至少可以持平，线上效果相差应该也符合预期。最开始的时候我们采用的方法也相对简单粗暴，将一些特征离散化之前的原始值直接作为deep侧的输入。 简单粗暴的结果当然是不尽如人意的，AUC要比预期低不少。而且单独测试，deep部分的AUC要明显低于wide部分。经过分析发现，之前我们的大量经历花费在「创造」新的特征上面，忽略了很多已有特征的监控和维护，很多特征已经不再准确（代码我都没改，怎么可能有新Bug呢？），但因为离散化，加上特征本身的稀疏性，这些问题并没有暴露出来。我们重新梳理了特征，并对输入wide侧的特征重新做了normalize。在这之后deep和wide分别单独测试的auc的gap已经在1%以内。 经过上面一些操作，此时WDL模型AUC已经相对于同时期的FM持平，线上实验也基本上无差异，但这并不是我们最终想要的效果。既然针对CTR这个任务预估而言，两个模型带来的结果是一样的，区别仅仅局限在输入上的差异，那么这些差异（deep侧）究竟给我们带来了什么？$$P(Y = 1|x) = σ(w^T_{wide}[x, φ(x)] + w^T_{deep}a^{(l_f)}+b)$$回过头来看WDL公式本身，logits就是deep和wide两部分logits之和。我们很直观的一个想法就是分别看deep和wide部分对最终logits贡献。 merge_epoch 上图是两部分logits的分布，结果也很直观：得分基本由LR部分决定，DNN部分得分对最终结果的影响十分微弱并且集中在0附近。并且随着训练进行，这种情况会越来越明显。最后都只剩下LR了还想着要啥自行车（ 经过几轮的简单的参数调整后，我们发现修改模型参数无法对这个情况产生影响。既然DNN在和LR联合训练尚不能取得比较好的效果情况下，那么可否先对DNN进行预训练，在此基础只上训练WDL？在拆分训练之后，DNN部分的logits符合预期。 遗憾的是到这个时候线上效果依旧和base持平，但这时候我们也意识到WDL对比base，下限取决于你的wide部分，而上限则是deep侧决定的。之前都没试过DNN，就想着一下在WDL上取得成绩，可能不如把问题简化，现在DNN部分上拿到收益，再和LR叠加提升收益。 2.2 DNN大力出奇迹。DNN提升的来源似乎可以用上面几个字概括。在调优DNN方面，首先是使用FM之前全部特征，在经过调整后，AUC和FM的差距大约有0.5%。吸取了之前的经验，我们依旧怀疑引入的特征有问题（这特征有毒，使不得）。 Permutation Test 似乎是一个帮助我们分析特征的一个好方法，简单粗暴。我们随机打乱样本中某一个特征的顺序，重新训练模型。如果这个特征越重要，那么在打乱之后模型效果也就越差。看起来行的通，但我们忽略了这个方法的一个大前提：特征之间需要相互独立，比如一个酒店的房间数量往往隐含着房间的价格、入住客人数量以及房间配置信息，这时候我们通过随机打乱的方式改变其中一个特征很可能构建出真实世界并不存在的样本，基于这种情况去分析特征表现往往会有很大的误导性。虽然shffle特征后对模型AUC带来的5%或者3%的变化是不可比较的，但是0和非0是明确有意义的。最终我们通过这个方法，移除了那些在shuffle后对效果影响不大的特征，避免输入更多的噪声。 离线AUC提高并不意味着线上表现一定提升。当DNN对比FM提升1.3%的情况下，我们线上表现依旧低了2%。这个时候单从离线的角度上已经不那么好发现问题了。粗略总结可能有以下几种情况： 低级错误 这个我们也是最开始排查的，常见的有（我们遇到过的，虽然拿出来讲感觉有点丢人，但是操作起来真的会遇到，或是自己的锅，或是队友的，第一条写在这就是提醒自己。排查问题的思路要清晰，不要总把问题想的太复杂，就以目前的水平能遇到啥别人没遇到过复杂问题呀，遇到了也不一定是自己能搞定的）：服务超时，线上特征抽取与离线不一致，特征穿越。排查后并不存在这些问题。 历史模型存在偏置 我们的训练语料采用的的是线上旧的模型排序/展现结果，点击label=1，未点击label=0。T时刻用于训练，T+1时刻用于测试。所以训练语料实际上是基于模型A的结果topK进行训练，线上预测同样是在召回N中取topK。当线上模型B和A的结构相近或者改动较小时候，偏置不会产生很大问题。但当模型差异较大的时候可能会存在类似「过拟合」的情况。 样本分布差异 新旧样本分布差异。线下训练的时候使用的都是已知的样本集合，而线上环境中会不断有新样本产生，这里我们针对T和T+1时刻数据切分已经包含了这种情况。 数据采样带来的差异。一般我们构造训练数据的时候都会依照业务场景做一些采样，这也就意味着离线和线上输出分布存在着认为构成的差异，但这种采样与模型无关。 评估指标 AUC计算的时候会将所有样本纳入计算，线下AUC提升为什么不能带来线上效果提升这篇文章讲的很好理解，这里就引用（抄写）下：线下的AUC、PR、loss、OE等等一堆指标，评估的是 CTR(user*–ad*) 是否够准确，也即每条样本是否拟合准确。而线上真实环境下，关心的是ctr、cpm、cvr等实际收益，所以评估的是 CTR(user–ad*) 是否够准确，也即user1选哪个ad*，最后ctr更高。这导致了线上线下评估目标不对等，自然会存在差异。类似的例子：在LR中，我们添加与item无关，但与user相关，能够引入user bias的特征往往能带来离线AUC的提升，但线上系统并不会被我们的辛苦搬砖的行为所感动。 结合当时遇到的情况，那会我们还没有使用GAUC进行分析，我们将问题的矛头指向历史模型存在偏置，但导致这种现象的本质原因是什么呢？我们通过观察业务指标发现，在新模型上的热门数据展现占比要比base低26%。我们语料在调整上更倾向点击用户，我们的业务场景也是一个满足二八定律的场景。在离线情况下给用户在一定比例热门数据的基础上推荐更具有多样性的数据会促使点击提升。但在另一个全召回的分布下，给用户推荐多样性更好的数据会导致效果变差。同时，冷用户也需要更多热门数据来提升表现。模型更多的memorize了给定的数据集合，没有学到我们想要它具有的泛化性。这些同时指向了item侧特征构建，item侧特征信息表达不够充分。 分析整个特征处理的过程会发现，之前在FM模型中，很多高维稀疏特征都是one-hot编码后直接使用，在DNN中，我们针对这类特征我们在编码后将其转换成了embedding_column从one-hot到embedding随着embedding维度的减小，带来的信息损失也是越来越大的。在dense column的限制下，我们embedding_size &lt;&lt; one_hot编码维度的。我们在我们能接受的训练时间和模型大小的前提下，使用了尽可能大的embedding_size。这一改变是明显的，模型大小直接提升了数倍，带来的效果也是明显的，AUC在提升1%左右的情况下，线上效果提升了2%。而我们做的仅仅是暴力的调大了几个特征列的大小，最终业务上的热门数据占比则降低了5%左右。这也让我们理解了浅层网络的宽度更重要。单从模型参数维度考虑，假设某个ID类特征共有N个取值，第一层embedding大小位e，后面接一个隐层，维度为d，则模型参数为N*e+e*d，对于常见的高维系数特征而言，N&gt;&gt;d，可见修改e的值会带来模型参数数量级的影响。 2.3 PCTR PCTR这部分可能存在一些不那么政治正确，不当讲的事情 上半年的时候公司商业化团队在公司相关类似的团队以合作的方式推广他们的PCTR–ctr预估模型及其整套的计算框架，最初合作的方式是他们那边出几个人，使用我们的数据，用他们的框架，承接我们的线上流量（野蛮人过来抢业务，挑战你，说的无私一点就是大家都想问了把公司的业务做的更好，也不是说部门墙，我们不开放，但强行赛马，真的更好么？当然这些干我屁事啊？中间又经历了啥我也不知道。主要的一点是这个框架也是他们某总监从前东家带过来的，简单沟通之后发现也是属于吃老本，不具有维护能力，同时也不想继续开发，这就贼他么尴尬了，我们也很难接啊也就理解他们的难处了，自己没办法继续搞了，只能拿着这个东西去抢业务了。）。最终在领导们的努力下，变成了他们帮我们搭建这套框架，并交付给我们自己维护，具体效果我们自己负责，留下了一堆安装文档，及一套对我们闭源的框架。（题外话，据说这个框架是百度的，两位从相关团队离职的人，分别单人了两家公司相关业务的总监。) 嗯，不管怎么说，这套框架放到我们的业务上也确实取得了一定提升（然而都是大公司几年前的东西了）。 具体框架层面：内部实现了参数服务器，支持稀疏和稠密参数，并行、异步无锁更新。训练模块采用插件化的方式实现，但仅仅包含了LR、MF、DNN，还有一个CVM（生成show、click等连续值）插件。 结构：分布式多进程，都是平等的worker，互为server。每个worker内多线程。每个worker都会负责：1)读取部分数据、2)shuffle、3)balance、4)训练、5)存储并更新部分参数 通信方式：获取及更新参数：异步RPC，基于zmq实现 统计耗时、内存等系统指标，进程间同步：MPI 明人不说暗话，这些都是文档上抄来的。接下来就是模型部分了，一图胜千言。 pctr 训练流程： 使用一天或一个pass的所有instance先进左侧模型，只更新dnn中的参数。 所有instance进右侧模型，更新dnn中的参数和LR、MF中的参数，以及增减特征。 双模型是整个模型比较有特点的地方，但仔细想下也比较好理解：embedding端到端训练完成后，在增量更新时候锁定。使得embedding后面的网络更容易训练，同时模型也更加稳定，可以认为后面的DNN是对特征embeeding进行fine-tuning。 除了update策略上有些创新外，模型还强调了对输入的数据要进行充分的shuffle，这个可以理解，我们数据往往基于日志构建，同一个用户同一次请求的数据往往集中在一起，充分shuffle后有助于提升模型收敛速度，加速模型训练。 CVM统计的是特征的show_click信息，如果配置了LR、MF、CVM，则每个feature，会有1维的LR的weight，8维的MFweight，1维SHOW的统计值，1维click的统计值。在backward中更新lr、fm参数时同时统计show、clk。这个模块相当于变相进行了一次特征筛选工作。另外，show、click计数部分也引入了时效性的概念，show、click会随时间衰减。具体来讲，就是在每天所有数据训练完后，分别对各个特征的show、click乘一衰减系数。 一些问题： 两侧的模型有何区别？ 左侧模型是用于训练线上所用的dnn网络（线上只使用左侧的dnn网络）。对于它来说，输入是LR、MF等数值。右侧模型是用于训练供左侧用的LR、MF等数值，相当于在做embedding。对于它来说，输入是one-hot编码的特征。 为什么先走左侧后走右侧？ 右侧模型的训练其实是独立的，不管先走左侧还是先走右侧，它的训练结果都一样。左侧模型的训练会受到顺序的影响：若先走左侧，则它用的输入是右侧模型用T-1时刻样本训练出来的embedding；若先走右侧，则它用的输入是右侧模型用T时刻样本训练出来的embedding。线上场景中，是使用T-1时刻的输入，预测T时刻的样本。离线训练时，其输入应与线上场景保持一样，才能取得较好的训练效果。所以，应先走左侧。反证：假设先走右侧模型，则LR、MF特征已经被更新，学到了新instance的信息，再训练左侧就有偷看答案的嫌疑；且左侧训练时不符合当时线上的情景，训练完后，再次还原线上过程让它使用T-1时刻的输入，预测T时刻的样本，不一定能取得很好的效果。 这个模型我们只是准备好我们自己的语料，和相关的同事一起配合支持了我们的线上业务，就模型优化层面我们并没有更多深入的理解，在cvm做特征筛选部分我们觉得是个相对可以借鉴的点，但是受限于我们使用的tensorflow，在技术层面很难实现框架里的cvm插件，这条后续路线也就到此结束了。 除此之外，这个框架通过双模型的方式让我我对embedding有了新的理解，其双模型主要想要实现的是对embedding进行冻结。embedding在推荐结果中是针对历史信息的一种表征，需要大量语料来学习。在一段时间内（pctr的假定是1天）embedding的语义信息是不会发生较大变化，比如出国游一词在假期来临的时候其关注度会被提高，但归结原因并不是出国游 一词本身语义发生变化，能带来更高点击，而是假期到了，出游是假期的一种选择，我们把embedding冻结起来，模型能够更加快速的捕捉到其它特征权重的变化，从而进一步提升模型的泛化性。另一个例子，提到隐形的翅膀 这个词，其背后含义很可能代表着张韶涵的一首歌曲，但作为一首十余年前的歌曲，突然有一天被腾格尔的翻唱刷屏洗脑，热度迅速提高。这时候隐形的翅膀其背后的语义对比过去一段时间已经发生了巨大的变化，这时候其代表的含义可能已经变成了洗脑歌曲，腾格尔翻唱。这时候我们再去重新训练embedding就会有收益，所以也就有了更新embedding的过程。 2.4 DSIN 有样学样，照着别人的演进路径来自己实践。 2.4.1 DIEN当有了一个好的开头之后，后续就顺利了很多。 首先是参照阿里的DIEN模型，在模型中加入用户历史点击序列，通过Attention机制刻画当前item和用户历史点击之间的相关性，通过GRU建模用户兴趣演变。 在最开始也试着将DIEN里面的Auxiliary Loss引入到我们的模型中，但是离线实验了具体的效 果，并没有太大的提升，AUC提升不到0.1%。分析原因，可能是因为引入辅助loss的目的是指导 用户兴趣的学习(在输入侧通过构建正负样本对)，但是在我们的样本特征里面，已经有了用户 embedding相关的中间特征，这部分特征描述了用户不感兴趣的item。 模型使用了3天的语料rebase，设置epoch=6，离线AUC相比FM提升1.15%，线上ctr大概提升了 3.5%。 一个问题：一般包括RNN在内的大多数时间序列模型都有一个前提，就是各输入之间是等时间间距的，但是在我们的NN1里面，点击序列之间并不是等时间间距，这样做没有影响吗？ 其实这个我们有做过考虑，我们尝试了将时间间隔作为bias加入到我们的每一个点击item，各种方 案都有所尝试，但是却没什么效果，估计也跟这种特殊的行为序列内在数据模式有关。 2.4.2 引入自适应激活函数激活函数方面文中提到了Dice激活函数，在这之前，对比ReLU和PReLU，没有太大差异。$$PReLU(s)=p(s) \\cdot (1-p(s)) \\cdot \\alpha s, p(s)=I(s&gt;0)$$ $$Dice(s)=p(s) \\cdot (1-p(s)) \\cdot \\alpha s, p(s)=\\frac{1}{1+exp(\\frac{s-E(s)}{\\sqrt{Var(s)+\\epsilon}})}$$ 可以看到，Dice是对PRelu做了平滑，使得拐点不再是固定的0，而是依赖于数据的分布。将Dice运用到我们的模型，AUC大概能提升0.4%，通过模型参数调节和引入Dice激活函数，线上指标大概能提升1.5%。 2.4.3 引入历史60天点击session在使用GRU对用户点击序列建模的过程中，发现历史长度最大取10和最大取100，离线的AUC变化大概在0.2%左右，但是耗时却增加了不少。考虑到模型上线的问题，最后只选取了最近的10次点击。考虑在我们的实际业务中，用户在一段时间可能只对某些主题感兴趣，我们可以将用户的历史点击划分为多个session，然后对每个session去提取相关的topic embedding，这样可以将序列的长度取得更长，同时也不会增加多少耗时。在综合session个数和session内的点击次数后，选取了用户最近的10次点击session，session长度最大取20，这样就可以覆盖200次点击行为。模型的结构如图所示，sessionembedding使用简单的avg pooling方式，离线AUC大概提升了0.275% 图中Attention sum计算方式如下：$$V_{u}=\\sum_{i=0}^{N} w_iv_i=\\sum_{i=0}^{N}g(V_i,V_a)V_i$$其中g()表示Attention计算函数，V_i 表示历史点击，V_a表示目标item。 2.4.4 使用multi-head self attention表征session前面的模型非常简单粗暴的使用Avg Pooling来表征session，无法刻画session内行为间的相关性，同时session内可能也有噪声，有些点击是偏离topic。为了解决这个问题，使用Multi-head self-attention对每个session建模，同时为了刻画不同session间的顺序，使用Bias Encoding，离线测试AUC大概能提升0.22%。模型设计参照DSIN实现，模型结构如下： 2.4.5 引入BiLSTM刻画session演变 由于性能问题，我们只能对10次点击的历史使用GRU序列建模，在经过session embedding后，我们可以直接对session的序列建模，捕捉不同session间的演变，而session包含了用户最近200次的点击信息，这样就丰富了我们的序列信息。通过引入60天点击session、multi-head self attention、以及的迭代，离线AUC提升了0.551%，线上指标提升2%。 3. 语料调整 语料调整的过程最开始还是以FM模型为基础实现的，后续切换到NN模型后基本都保留了下来，除了少许改动。 这部分内容纯属一个工程经验以及业务理解部分，比较蛋疼的地方是涉及到语料调整的实验，只能看线上AB后验，调整后的模型在离线预测上因为训练和测试已经不再在同一分布上了，所以离线验证auc会有一些抖动。 语料精简，顾名思义，在这里是丢掉不需要的语料。这个场景是否可以有有效曝光的打点？如果有，那么果断用有效曝光，效果不会让你失望，在我们尝试后有些渠道就把全量展现切换到有效曝光后就可以有10%左右的提升！经验可以概括为：若果展现pv和有效曝光的差距越大，那么切换到有效曝光后，提升也就约大。这也很好理解，我们引擎分发出去的数据因为各种展现样式，屏幕尺寸，广告遮挡的原因导致用户没有看到数据的情况下，我们将样本标记为负样本—-这并不代表用户真实行为。这样的数据越多，导致我们和真实的数据分布差异也就越大。 换言之，我们所有渠道都只使用用户有效曝光数据不就可以了？但实际上我们的业务场景里并不能做到所有场景都有有效曝光，比如我们在手机浏览器的信息流服务，手机浏览器团队就明确拒绝我们这样的打点需求—-耗电。所以我们后续做的也很明确，理解业务场景通过策略调整语料，让语料更加接近有效曝光的分布。 好了，如果理解了上面的那句话，这里似乎就没有什么好讲的了。 使用5分钟内有点击用户的全部样本进行增量训练，同比全采样，点击提升4%。 使用3天数据init模型，5分钟增量更新模型，ctr提升1%-2.4%，点击提升0.5%-2% 7天init后，不再对模型rebase，一直增量更新。ftrl更新，如果长期不初始化，稀疏性越来越差，到一定时间后模型效果开始变差。 5分钟最后一次点击前语料，丢弃疑似无效曝光，ctr提升1.2-2.6%，点击提升0.4%-2.1%。 5分钟最后一次点击前语料 ＋ 最近72小时语料，ctr提升3.4%，点击提升2.8%。 保留请求间隔大于8s数据，ctr和导出均下跌，说明该方式误差较大。 修正展现位置偏差，最大位置ctr/不同位置的比值加权点击 × k 或者只取topk的数据（指标持平） 过滤ctr&gt;s的用户数据，点击降低1.2% 对展现量过高的头部的item对应的样本进行采样，点击降低1.4% 只取ctr&gt;s的item对应的数据参与训练，点击降低3% 4. 一些其他的问题（待补充） AUC -&gt; GAUC双峰问题","categories":[],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://yoursite.com/tags/推荐系统/"}]},{"title":"Applying Deep Learning To Airbnb Search","slug":"「阅读笔记」Applying-Deep-Learning-To-Airbnb-Search","date":"2019-05-13T02:58:21.000Z","updated":"2019-05-15T16:28:00.102Z","comments":true,"path":"posts/2019/05/13/62440/","link":"","permalink":"http://yoursite.com/posts/2019/05/13/62440/","excerpt":"之前阅读了了KDD2018上Airbnb发表的 Real-time Personalization using Embeddings for Search Ranking at Airbnb，第一感觉是这篇论文的可实操性非常强，值得一读，相关解读也非常多了。也正式因为上面那篇文章，才又发现了这一篇文章。这篇文章记录了整个Airbnb搜索推荐从传统机器学习模型到深度模型的演进与探索，深入浅出，同时能把很多失败的尝试暴露出来。对于深度学习落地到推荐在线排序来言，基于我目前经验，我觉得还是十分值得一读的。","text":"之前阅读了了KDD2018上Airbnb发表的 Real-time Personalization using Embeddings for Search Ranking at Airbnb，第一感觉是这篇论文的可实操性非常强，值得一读，相关解读也非常多了。也正式因为上面那篇文章，才又发现了这一篇文章。这篇文章记录了整个Airbnb搜索推荐从传统机器学习模型到深度模型的演进与探索，深入浅出，同时能把很多失败的尝试暴露出来。对于深度学习落地到推荐在线排序来言，基于我目前经验，我觉得还是十分值得一读的。 原文地址：https://arxiv.org/pdf/1810.09591.pdf 1. 模型演进从2017年4月到2018年6月之间共计四次比较大的迭代，离线使用NDCG进行评估。线上使用订单总数作为指标。图中y轴表示相对提升。 GBDT是最开始的base 除此之外还有两个有趣的点：1. NDCG的提升幅度和线上最终表现存在差异，并不是说离线评估提升越多，线上表现越好。2. 从Simple NN 到 Lambdarank NN 包含取得确定收益只花费了2个月，而从Lambdaran NN 到 GBDT/FM NN 花费了9个月之久。这部分收益取得的代价可谓是很高了。 1.1 simple NN最终上线的模型是一只包含一个隐层的NN模型，32个ReLU unites，特征沿用之前GBDT模型的特征。训练目标也和之前GBDT保持一致，将用户最终预定的item标注为1.0，没有预定的标注为0，最小化L2 regression loss。 讲道理这个模型这么简单确实没啥好说的，真是的Simple NN。但文中交代了他们最开始也想尝试自定义复杂的结构（Why can’t we be heroes?），但是最终在复杂度面前妥协了下来。最终他们选择这个简单的模型来验证整个改造后的系统流程，达到一个完备状态并且可以承接线上流量（我理解的是离线指标和线上指标至少和base持平，解决中间的各种工程问题），效果上有小幅度提升。个人认为这里最值得讲的就是决策执行，发现问题行不通赶紧切换方向小步快跑，由浅入深，先以简单的方式将流程打通再深入优化。我们团队吃过类似的亏（换句话说就是想一口就吃个胖子），引入NN尝试的第一个模型便是阿里妈妈提出的DIN模型，操作了2个月有余还无法和base持平，最终放弃（公司里另外一个尝试DIN的团队也没取得论文中提到的那么大的收益）。后来也是先使用简单的模型将整个流程跑通，排查各种问题。解决生产问题最为优先，这算是是工业界和学术界的差异了。 1.2 Lambdarank NNLabmda Rnak可以直接针对NDCG进行优化，正因如此，在将NN和Lambda Rank相结合的时候带来了第一个突破。主要改进有以下两方面： 改用pairwise损失，构建{booked listing, not-booked listing}作为训练样本。训练过程中最小化booked listing和not-booked listing得分差值的交叉熵损失。 通过对调listing带来的NDCG的差异作为pairwise loss的权重，这样可以使得NN直接针对NDCG进行优化，同时模型优先优化排序靠前的listing。listing position从2到1收益要高于从10到9。代码实现： 这里通过改变不同pair对的权重来优化效果可以理解，但是不明白是的最终权重的选取为什么是NDCG的变化值？ 这点暂时还不是很懂。正好后面也要尝试pairwise loss。这里暂时先留个坑后面再来补充吧 1.3 Decision Tree/Factorization Machine NN在线上使用NN模型的同时，作者还探索了一些其它模型，例如GBDT和FM。GBDT上是使用一些可替代的方法简化搜索，用以构建训练数据。FM是对每个召回的listing进行预订率的预估，使用了32维的隐向量。比较有趣的是，虽然这两个模型在测试集上表现都和线上的NN模型相近，但是排序结果的头部却十分不同。因此他们尝试将三个模型进行融合：直接将GBDT每个树的预测结果的在叶子结点中的index作为类别特征放入NN中，FM部分则是直接取预估值作为特征放入NN中。模结构如下： 1.4 Deep NN上面模型的复杂度令人惊愕。但在尝试将训练数据变成原来的10倍，将模型变成一个简单的只包含两个隐层的DNN模型后，之前的复杂尝试都可以抛弃了。模型很简单，输入层包含195维特征，两个隐层，分别是127和83个ReLU unites。在他们的场景下他们大约使用了17亿的语料（pair对），才使得模型达到比较好的泛化效果。输入到模型里的特征大都是listing的基础属性，比如价格、设施、历史预定数等等，这些特征通过尽可能少的特征工程后直接输入到模型中。除此外还有一些其它模型产出的特征：智能定价模型输出的价格特征和embedding的相似度特征，因为这些模型使用的的数据并不是直接参与rank模型训练的样本，从而能为DNN模型带来一些外源信息。 ?文章里还表达了一些疑问，比如在cv领域，可以通过人工标注的方式来定义人类水平，从而可以和模型效果进行比较。但这种推荐的业务场景下，日志里并不能反应真正的客观事实，有很多隐藏的信息我们无法捕捉到，很难进行人工评测。这里我简直不能再认同！比如图像识别，文本分类这种都有明确的客观答案，我们是可以通过看case来发现问题的，从而优化模型效果。但推荐这种场景，我怎么知道你最终为啥没点击/预定？（没准上分钟在看房，下一分钟你朋友打电话告诉你他刚在那买了新的房子，你可以过去住！尽管你已经在输入支付密码了，你还是有可能取消） 我在我们的场景下也看过若干case（迫于年轻），也看不出啥来，而且只能看很少一部分，很难给出一些明确的结论，即便有也只能是一些泛泛而谈的屁话。我们也有一些业务上的专家试图从case上发现问题，但在我看来往往都带有很强的主观色彩，并不能使我信服，而且更严重的这种主观上的你觉得推荐结果有问题，鸡蛋里总是能挑出骨头的。 2. 一些失败的尝试作者在这里讲了一些他们并没有取得成功的尝试，中间过程也值得我们反思。 2.1 Listing ID通过往模型中加入高维稀疏的类别特征，比如video_id和user_id，在一些NLP任务和推荐系统中都取得了提升，所作者试图使用Listing的唯一ID作为特征的输入，使用模型直接构建embedding来表达这个ID背后所独有的特征。然而作者在多种尝试后发现，加入Listing id 非常容易使模型过拟合。虽然id特征在训练集上能带来显著的提升，但是在测试集上却没有这种变化。 作者认为这个已经被别人验证有效的方法在这里行不通的原因主要和业务形态有关系。embedding需要一个同个item有大量的语料来给出适当的取值，这在NLP任务或者视频推荐任务中都是可行的，同一个单词或者视频都可以大量重出现。但在他们的场景下，Listing受到真实世界的物理制约，同一个Listing只能出现在一个市场，同时每天只能被预定一次，一年至多只能被预定365次。然而典型情况下一个房间被预定的次数往往更少，同个Listing数据非常稀疏，这直接导致了模型过拟合。 2.2 Multi-task learning虽然预定这个操作受到物理因素的限制，但是详情页浏览缺不受到这种约束（这里我也没明白作者想表达什么意思… 大概需要理解他们的业务背景）。他们发现了long view（这里应该是详情页长停留时长？）和种种的booking ratio之间存在某些关联。他们认为这是因为某些listing比较稀疏导致的过拟合。作者尝试使用Multi-task learning的方式对booking probability和long view同时进行建模。 模型结构如上图，两部分输出使用独立的输出层，共享隐层。预期是模型能够从学习到从long view到bookings的转换从而来避免overfitting。由于long view的数量和booking的数量有着巨大的差异，所以重新设计了损失函数，所以针对booking loss给了很高的权重来使得模型保持针对booking的预测。long view用log(view_duration)进行scale。线上排序过程中，仅针对booking进行预测。然而在线上实验发现，booking没有提升，但是long view的比例缺大大提升了。通过手动查阅那些Long view和booking的case发现有一下一些原因可能导致这些问题：listing非常高端，同时价钱也很高；详情页描述信息冗长；十分特殊或者搞笑的描述以及种种其他原因。long view和booking之间的关联也是Airbnb业务上的独特之处，但在一些场景下却没什么相关性，这也使得基于Long view的booking预测模型十分具有挑战性。这些成为了他们接下来要深入研究的一个问题。 从业务中发现问题，同时再将业务问题用模型的思路去解。也许同样的方法别人能够取得非常好的效果，但在自己的场景下可能并带不来指标上的提升，这不是因为工程没有做好，很可能是因为业务上存在的特殊性导致。而这些除了亲自采坑之外，只能结合业务场景和技术内部原理进行预判了（要求不是一般的高） 3. 特征工程在DBDT的基础上，他们做了大量的特征工程工作，在这些特征工程背后也暴露出了很多问题，在特征加入的模型时候一定是正确的，但随着多次迭代和整体业务的变化，我们很难去确定之前实现的特征在当前是否时候过时或者依旧最优。而NN一个十分吸引人的点就在于可以将他们从上面的困扰中解脱出来。虽然NN能够利用原始数据进行特征提取，但仍需要很多工作来保证模型足够高效。 3.1 Feature normalization最开始他们直接将GBDT的特征直接输入到NN的模型里，效果非常差。决策树这类模型对数值特征的确切值并不关心，只要保证大小顺序有意义。而NN模型对数值类特征大小十分敏感。特征scale不同会导致梯度变化巨大。较大的梯度更新会导致梯度消失，这会永久关闭ReLu这样的激活函数。为了解决这一问题，作者对特征进行了normalize，使特征分布在{-1, 1}的区间内，均值为0。针对不同类型的特征选择以下两种方法： 如果特征值在样本中类似于正态分布则使用（feature_val-μ)/σ 。其中μ和σ分别是特征的均值和标准差。 如果特征值在样本中类似于幂律分布则使用 log((1+feature_val)/(1+median))。 3.2 Feature distribution除了把特征缩放到一个较小的范围内，还要尽可能使特征分布更加平滑。作者解释了这样操作的具体原因：Spotting bugs. 在大量的样本中，如何确认某些特征的值是不是有问题？检查特征值的范围有效但效果有限。他们发现通过检查特征值分布对照典型分布是否足够平滑是查找bug的一个好方法。特征值分布图上的峰值很有可能就是bug导致。Facilitating generalization. 作者基于他们的观察发现，在他们的DNN模型中， 隐层的输出值会变得越来越平滑。作者把隐层中的值取出来，忽略掉零值，并且使用log(1+relu_output)进行变换后，分布如下： 这些图也给了作者一些启发：DNN之所以能在当前场景下有着不错的泛化效果，是因为构建的模型特征很多，特征组合的空间会非常大，在训练过程中模型也只能cover一部分信息。在较低的隐层中特征分布越加平滑，保证了在后面的隐层能更好的响应那些未见过的特征值。因为，作者尝试最大限度的让输入层的特征分布平滑。除了进行线上测试外，作者还找到了一个有效的离线检测的方法。他们针对测试集里的某个特征进行缩小或者放大，然后观察测试集上的NDCG变化情况。最终发现NDCG非常稳定，也就是说模型能够在未见过的样本上表现同样稳定。大部分特征在进行排错和适当的标准化后，其分布都会变得平滑。但是有些特殊的特征仍旧需要进行一些处理，比如list的geo信息。下面两个图是list的纬度和精度的原始值分布情况。 可见，经纬度的分布非常不平滑。为了使得特征变得更加平滑，作者计算了list实际位置到用户看到的地图中心的偏移量。变成了下面这样。 看起来数据在图中部十分集中，是因为地图尾部数据导致数据被缩放。再进一步使用log()对经纬度进行变换。 其特征值对应的分布也变得平滑许多。 需要注意的是，在这里的针对经纬度进行的一系列变换是有损的，其会将多个位置映射到相同的偏移量上，这使得模型能够学习到基于距离的全局特征，而不是一些基于特定地理位置的特征。至于特定地理位置的特征，则使用高基数的类别特征进行刻画。Check feature completeness. 在做一些场景下，分析那些不平滑的特征也能帮助我们发现模型缺失的特征。作者举了个例子，在他们的场景下有个未来可预定天数这样一个特征来刻画list的质量。但是这个值分布不平滑。结合业务发现有些list有最短预定时间的要求，这直接影响了这一特征。但是他们的做法是并没有直接将最短停留时间和这个特征放到模型中，他们认为这个特征和日期相关，设计起来非常复杂。他们使用了list的平均预定天数对这个特征进行了标准化。最终特征在新的这个高维空间上变得平滑。 k 3.3 High cardinality categorical features除了listing_id外，作者还尝试了其他类似的特征。其中有些通过简单的特征加工就在NN里取得了比较不错的效果，比如街道信息。在之前的GBDT模型中需要有一个复杂的流程来追踪社区和城市的预定等级分布。然而在NN模型中，交给模型自己处理这类信息是非常简单的，他们通过将用户query和listing的位置信息组合创建了一个新的类别特征。以San Francisco为例，通过level 12的S2编码，一个Embarcadero附近的listing，其位置信息可以标识为(539058204)，然后通过某种hash方式可以将{“San Francisco”, 539058204}表示为71829521。然后直接使用这个hash后的数值来构建类别特征。这个数值可以作为embedding信息输入到NN。在模型训练过程中，模型能够捕捉到embedding背后所编码进去的S2 cell和query信息。可以从下图中看出，针对给出的San Francisco这个例子来说，模型学习的效果符合他们预期：高亮的点除了标记处城市中受欢迎的的位置外，同时还表明了对更远的位置的偏好，而不是最靠近大桥的位置，因为那里是交通最拥堵的地方。 l 4. 系统工程这部分简单介绍了他们的系统流程和中间遇到的一些坑，服务端使用java编写，主要负责召回和排序两件事，同时把日志保存下来用于离线训练，使用Thrift对原始数据进行序列化。使用spark对数据进行处理，构建训练语料。使用TensorFlow训练模型，离线的统计/评估脚本大多使用Java/Scala进行编写。最终模型会放在Java的服务端进行使用。Protobufs and Dataset. 他们之前的GBDT的训练流程使用的都是csv格式的语料。在迁移到TensorFlow的时候尽可能多的复用了以前的流程。所以最开始他们使用了feed_dict这个api输入数据（众所周知，这个api实在是慢），也忽略效率了这一问题。等他们发现他们机器的GPU的利用率只有25 %不到的时候他们才反应过来（偷笑）他们大部分训练时间都话费在了解析数据上。作者用「骡子拖法拉利」来形容他们当时的状态。最终他们改用Protobuf组织数据，并使用data_set后，训练速度提升了17倍，GPU利用率也提升到了90，最终使他们的训练语料的大小从数周提升到了数月。Refactoring static features. 在Airbnb的场景下，listing有很多特征很少发生变化，比如房间数目，地理位置等，在构建样本的时候提取这些特征会造成一些输入上的瓶颈。为了解决这一问题，他们仅仅将Listing_id作为一个类别特征使用，通过Listing_id构建了一个不可训练的embedding，试图通过这个embedding表达所有这些近乎静态的特征。这样一来，降低了从硬盘上读取数据的大小，从而可以探索更多用户的历史行为数据。Java NN laibrary. 最早从2017年开始，他们就试图将TensorFlow的模型放到产品里使用。如果使用调用其他编程语言的库，数据转换会耗费大量时间，无法满足线上排序对延时的要求，所以他们使用java实现了线上排序部分。 作者他们遇到的工程问题可以说是很真实了……（我们踩了同样的坑。我们遇到的坑除了训练效率问题外，还有一点就是我们在小流量实践NN模型的时候，线上还保留了旧版本的模型，我们需要复用同一个pipeline，换言之我们在日志中序列化保存的数据是另外一种中间结果，我们离线去做一些转换，来同时支持我们NN模型和原有模型的训练。因为保存的中间结果，线上服务使用C++编写，离线格式转NN使用的是python，相同的逻辑用不同语言实现了两次，导致这里曾经出现了一些不一致的地方，后续排查问题的时候也带来了一些困扰，在系统设计初期应该尽量避免这些问题。另外，我们这边线上排序的时间也是十分有限，大约100ms，使用TF Serving数据传输耗时大约在10ms左右。目前采取的方案是，实验模型使用TF Serving进行预测，待模型稳定后，使用C++代码实现模型线上排序部分以降低耗时。 5. 超参数在最开始的时候，作者他们也曾花费大量时间调整模型参数，虽然并没有带来太多实质上的提升，但解决了他们的措施恐惧症（Fear of missing out），同时也让他们在后面做出选择的时候更加自信。Dropout. 最开始作者他们以为NN中的必须添加dropout以起到正则化的作用。在他们的场景下，作者进行多种尝试后，发现dropout最终都会导致他们离线评估指标轻微下降。作者认为，dropout更像是一种数据增强变换：一些随机的数据缺能够模仿到训练集中数据缺失的数据，这会帮助模型提升效果。但是在他们的场景下，这种随机的数据缺只会产生很多无效的情况，反而会分散模型的注意力，使模型效果变差。作者他们找到一种可供替代的解决方案，他们针对一些特定特征的分布，手工构造了一些噪声，最终可以帮助模型在离线NDCG上提升接近1%。但遗憾的是，这些改进在线上并没有显著效果。Initialization. 纯粹出于习惯，作者尝试使用了全零初始化训练模型，最终发现这是一个非常糟糕的方法（笑）。在他们调研了多种方法后，他们选择了Xavir对参数进行初始化，并且将embedding的区间统一在{-1，1}。Learning rate. 学习率可调整的区间非常宽泛，但在他们的场景下，作者发现Adam的默认值效果已经非常好了。最终他们使用的是LazyAdamOptimizer，这个优化器在训练较大的embedding的时候速度更快。Batch size. 不同的Batch size会对模型的训练速度产生不同的影响，至于对模型本身的影响也难以理解，作者抛出了 https://arxiv.org/abs/1711.00489 这篇文章，算是他们可找到的为数不多的参考。他们也没有完全按照那边文章里的建议，最终他们只是将Batch size固定到200来训练模型。 6. 特征权重作者他们在尝试理解特征在NN里表现效果上也做了一些尝试，分析特征的重要新对于模型迭代和工程设计都非常重要。虽然NN在特征间的非线性关系计算上非常强大，但这也是的我们在分析任何单独特征的时候变得非常困难。Score Decomposition. 在之前的GBDT上，作者使用Partial Dependence Plots进行分析。他们刚开始的时候进行了一些「naive attempt」，尝试对模型最终得分进行分解，来分析每个输入节点对结果的贡献，但因为有ReLU这种非线性的激活函数存在，这基本无法实现。Ablation Test. 这个方法是通过每次去掉一个特征，使用剩余的特征重新训练模型，用新旧模型性能差异比例来表示特征的效果。作者他们在这里也有些困惑：通过随机丢掉一个特征获得的性能差异和重新训练时候在离线指标上观察到的噪声十分类似（我理解是离线评估变化都很微弱。）这可能是由于他们的特征经过一系列工程后，模型能够弥补其中单个特征的缺失。作者也使用忒修斯悖论来评价这种方法：能不能一直从模型中去一个特征，声称模型效果没有变化？Permutation Test. 作者他们尝试使用随机置换测试集中某个特征取值的方式来分析特征权重。某个特征越重要，那么在打乱之后，预测结果会变得越差。然而这个测试却得到了一些荒谬的结果：在他们预测房间预订概率的场景下，房间数量被该方法认为是最重要的特征。他们忽略了这个方法的一个大前提，即特征间相互独立。房间数量背后往往隐含着房间价格，可以接待的客人数量，娱乐设施等等属性。单独对这个特征进行打乱会产生很多并不会在真实世界中存在的样本，在这些不存在的样本空间里分析出的特征重要性会有很严重的误导性。但这个测试方法也并不是一无是处：如果随机置换某个特征后预测效果没有发生变化，也正说明模型并不需要这个特征。TopBot Analysis. Topbot是top-bottom analyzer的缩写。他们针对一次query召回的样本进行排序，分别取rank排序在头部和尾部的数据，对比特征在这两组数据上的取值分布。这个比较可以明显的看到模型对某个特征在不同取值区间上的利用情况。如图所示，排序靠前的房间更倾向于更低的价钱，这表明模型对房间的价格敏感。然而不论头部或是尾部的房间，其在review count这个特征维度上分布差异不大，这也反映出这个版本的模型并没有很好的利用这一特征。 虽然他们找了很多方法去分析特征权重，但都没有拜托条件独立的困扰，似乎TopBot Analysis 算是一个可行的方法。 7. 总结作者坦然说，最开始他们受到各种深度学习成功案例的影响，对整个排序模型迁移到深度模型上来这件事太过乐观，觉得只要简单的把模型本身替换掉就能带来巨大的提升。所以最开始他们并没有任何收益，这也使得他们一度陷入到绝望的山谷。最开始，模型离线指标是下降的，这也让他们意识到迁移到深度模型要伴随着整个系统的扩展，需要从模型的角度重新思考。虽然深度模型表现已经超过GDBT，但因为GBDT在小的尺度上具有更好的可解释性，同时也更好操作，他们依旧会使用GBDT解决一些中等规模的问题。作者表示会真心推荐深度学习给其他人，不单单是因为深度模型在线上带来了巨大的提升，深度模型也改变了作者他们技术重心。在这之前，他们花费了大量精力在模型的特征工程方面，而深度学习使他们从中解脱出来。这也让他们在一个更高的层次上思考问题：怎么能够提升优化目标？是否已经准备表征了用户？从开始探索深度学习到发文，两年多时间，在作者看来他们才刚刚迈出了一小步。 （完） 原本也不是想做通篇的翻译。很多地方觉得很简单，没必要，但是真的实操起来确实会遇到各种各样的问题，况且不论是实操的人还是这个人的leader都对深度学习没有任何经验的时候，文章里讲到的坑，翻过的错很容易出现。比如你的老大想当然的问你为什么通过分数分解的方式分析每个特征权重。虽然现在已经9012年了，但是我们的推荐系统还是以FM为核心，当然出去吹逼都会说什么深度模型，倒头来还不就是在一个很小的流量上跑了跑，还是核心指标低了好几个点的情况下。现在面临的情况就是组里原本两个做这个方向的同学历时8个月，还没成功的将DNN和之前模型效果达成一致，现在还被砍掉了一个人。可能马上这个项目就又要被沉寂了吧…… 虽然现在我也没什么机限会去实操这些，经验有，不好总结什么，但文章的介绍的路径和细节上来看，作者也是很实在，踩过的坑，解决问题的思路都值得学习。总之，期待春天早日到来吧。","categories":[],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://yoursite.com/tags/推荐系统/"},{"name":"阅读笔记","slug":"阅读笔记","permalink":"http://yoursite.com/tags/阅读笔记/"}]}]}